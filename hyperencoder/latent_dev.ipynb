{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from safetensors.torch import load_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf_t = load_file(\"../data/Track00001/mix.safetensors\")\n",
    "latent = sf_t['latents']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 64, 1024])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "latent.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.9830, -1.2921, -0.9089,  ..., -2.7601, -1.5248, -1.6257])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latent[0,1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vector_quantize_pytorch import FSQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FSQ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantizer = FSQ(\n",
    "    levels = [8, 5, 5, 5], # len of this is the bottlneck dimension/channels -- product of total codes \n",
    "    channel_first=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "expected dimension of 4 but found dimension of 64",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mquantizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlatent\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Projects/pre_encode_audio/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Projects/pre_encode_audio/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Projects/pre_encode_audio/.venv/lib/python3.9/site-packages/torch/amp/autocast_mode.py:44\u001b[0m, in \u001b[0;36mautocast_decorator.<locals>.decorate_autocast\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_autocast\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m autocast_instance:\n\u001b[0;32m---> 44\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Projects/pre_encode_audio/.venv/lib/python3.9/site-packages/vector_quantize_pytorch/finite_scalar_quantization.py:179\u001b[0m, in \u001b[0;36mFSQ.forward\u001b[0;34m(self, z)\u001b[0m\n\u001b[1;32m    176\u001b[0m     z \u001b[38;5;241m=\u001b[39m rearrange(z, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb d ... -> b ... d\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    177\u001b[0m     z, ps \u001b[38;5;241m=\u001b[39m pack_one(z, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb * d\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 179\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m z\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdim, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexpected dimension of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdim\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m but found dimension of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mz\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    181\u001b[0m z \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproject_in(z)\n\u001b[1;32m    183\u001b[0m z \u001b[38;5;241m=\u001b[39m rearrange(z, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb n (c d) -> b n c d\u001b[39m\u001b[38;5;124m'\u001b[39m, c \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_codebooks)\n",
      "\u001b[0;31mAssertionError\u001b[0m: expected dimension of 4 but found dimension of 64"
     ]
    }
   ],
   "source": [
    "quantizer(latent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "x = torch.randn(1, 4, 500) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.3625,  0.7183,  0.5512,  ..., -1.5676,  0.3226,  0.1560],\n",
       "         [ 0.7164,  1.4226, -0.0531,  ...,  1.5733, -0.1926, -0.4056],\n",
       "         [-1.3205, -0.0372,  1.3850,  ..., -0.5245, -0.7346, -0.2232],\n",
       "         [ 0.2902,  0.4099, -0.3767,  ...,  0.4288, -0.8736, -1.1047]]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 500])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[-0.2500,  0.5000,  0.5000,  ..., -1.0000,  0.2500,  0.2500],\n",
       "          [ 0.5000,  1.0000,  0.0000,  ...,  1.0000,  0.0000, -0.5000],\n",
       "          [-1.0000,  0.0000,  1.0000,  ..., -0.5000, -0.5000,  0.0000],\n",
       "          [ 0.5000,  0.5000, -0.5000,  ...,  0.5000, -0.5000, -1.0000]]]),\n",
       " tensor([[627, 718, 382, 218, 386, 736, 610, 379, 839, 505, 253, 641, 702, 977,\n",
       "          250, 444, 581, 750, 673, 212, 681, 440, 185, 904, 649, 411, 248, 212,\n",
       "          558, 524, 486, 413,  12, 139, 726, 178, 108, 206, 424, 943, 278, 801,\n",
       "          190, 488, 537, 322, 657, 659, 264, 875, 310, 339, 994,  26, 846, 794,\n",
       "          816, 926, 542,  13, 185, 609, 707, 265,   4, 493, 471, 471, 502, 214,\n",
       "          371, 418,  73, 345, 201, 627, 751, 529, 308, 621, 580, 222, 906, 332,\n",
       "          545, 303, 350, 340, 224, 286, 519, 906, 593, 983,  99, 249, 191, 555,\n",
       "          293, 475,  83, 749, 457, 230, 344, 479, 673, 286, 305, 597, 817,  48,\n",
       "            9, 703, 280, 145, 797, 252, 137, 308,  58, 394, 700, 495, 775, 375,\n",
       "          107, 410,  55, 286, 682, 142, 974, 718,  17, 196, 985, 746, 310, 990,\n",
       "           81, 958, 758, 898, 451,  35, 836, 209, 663, 436, 653, 499, 487, 894,\n",
       "          612, 855, 251,  75, 492,  45, 641, 728,  93, 780, 108, 265, 204, 126,\n",
       "          272, 965, 247, 208, 665, 172, 346, 841, 520, 999,   1, 285, 695, 551,\n",
       "           86,  66, 836,   2, 575, 745, 501, 794, 871, 601, 292, 358, 601, 723,\n",
       "           15, 682, 150,  89, 190, 361, 753, 288, 358, 785, 212, 437, 274, 611,\n",
       "          617, 977, 273, 583, 420, 663, 923, 851, 571, 238, 200, 609, 125, 666,\n",
       "          401, 293, 935, 687, 745, 604, 670, 461, 778, 190, 590, 294, 631, 491,\n",
       "          632, 906, 363, 140, 286,  71, 516, 739, 224, 943, 487, 429, 493, 459,\n",
       "          902, 387, 125, 294,  90, 642, 258, 704, 625, 870, 969, 435, 655, 493,\n",
       "          678, 771, 673,  34, 493,  74, 476, 377, 186, 156, 241, 477, 145, 483,\n",
       "          699, 729, 340,  69, 747, 952, 626, 975, 447, 814, 738, 551,  22, 735,\n",
       "          817,  10, 805, 542, 622, 844,  58, 477, 230, 386, 714, 613, 300, 307,\n",
       "          777, 476, 658, 774, 378, 374, 211, 640,  41, 388, 802, 619, 185, 338,\n",
       "          906, 413, 324, 451, 558, 330, 644, 510, 775, 779, 204, 775, 133, 357,\n",
       "          243, 643, 782, 118, 738, 371, 518, 309, 190, 740, 356, 774, 975, 440,\n",
       "          385, 298, 712, 286, 466, 443, 753, 531, 502, 607, 679, 980,  93, 286,\n",
       "          125, 141, 491, 412, 465, 777, 306, 627, 682, 441, 421, 962, 607, 263,\n",
       "          226,  64, 529, 307, 843, 252, 430, 531, 787, 909, 746, 987, 582, 789,\n",
       "          718, 135, 352, 824, 425, 683, 262, 224, 646, 802, 350, 677, 283, 417,\n",
       "          316, 139, 546, 930, 309, 497, 157, 790, 545, 177, 363, 108, 917, 289,\n",
       "          266,  16, 736, 796, 951,  13, 954, 225, 691, 689, 795, 244, 559, 729,\n",
       "          473, 495, 302, 358,  54, 227, 714, 614, 564, 308, 135, 546, 850, 370,\n",
       "          614,  76, 485, 336,  42, 288, 719,  94, 548, 574, 610, 617, 672, 278,\n",
       "          772, 443, 353, 381, 507, 333, 111, 711, 688, 895, 460,  18, 624, 666,\n",
       "          256,  48, 409, 690, 171, 288, 911, 161, 516, 494, 278, 338, 298, 972,\n",
       "          349, 529, 675, 253, 638, 980, 742, 672, 261,  93]], dtype=torch.int32))"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quantizer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "xhat, indices = quantizer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.2500,  0.5000,  0.5000,  ..., -1.0000,  0.2500,  0.2500],\n",
       "         [ 0.5000,  1.0000,  0.0000,  ...,  1.0000,  0.0000, -0.5000],\n",
       "         [-1.0000,  0.0000,  1.0000,  ..., -0.5000, -0.5000,  0.0000],\n",
       "         [ 0.5000,  0.5000, -0.5000,  ...,  0.5000, -0.5000, -1.0000]]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.2500, -0.5000, -1.0000,  0.5000],\n",
       "         [ 0.2500, -0.5000,  0.5000, -0.5000],\n",
       "         [-0.2500, -1.0000, -0.5000,  0.5000],\n",
       "         ...,\n",
       "         [ 0.5000, -1.0000, -0.5000,  1.0000],\n",
       "         [ 0.0000,  0.5000,  1.0000, -1.0000],\n",
       "         [-0.7500, -1.0000, -1.0000,  1.0000]]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quantizer.indices_to_codes(indices)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
